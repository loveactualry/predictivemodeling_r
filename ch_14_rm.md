CH14
================
hyojunahn
2020년 4월 6일

R Markdown
----------

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

``` r
a = c(91, 20, 30, 67)
ex_14_a = matrix(a, nrow = 2, ncol = 2)
# test_14 = as_tibble(ex_14_a)
x1 <- data.frame(ex_14_a)
x1["colsum",] <- apply(x1, 2, sum)
x1$rowsum <- apply(x1, 1, sum)
x1
```

    ##         X1 X2 rowsum
    ## 1       91 30    121
    ## 2       20 67     87
    ## colsum 111 97    208

Including Plots
---------------

You can also embed plots, for example:

    ## [1] "C"

    ## [1] 0.3650412

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.

> 이론

-   현재는 하나의 지니계수만 구했지만 분할 알고리즘에서 모든 분기점에 대해 구하고 순도수치 최소화 분기 값 계산
    -   반복 되므로 중단 기준 필요
-   최대 깊이가 주어지는 트리 -&gt; 훈련 데이터 과적합 우려
-   복잡도 인수 -&gt; 순도 기준이 마지막 노드 값에 벌점이 매겨지는 방식이다.(8.1 참조) 8장가서 봐야지..

-   훈련된 트리를 예측에 사용.
-   만약예측 변수가 B=0.1로 한다면 클래스 1,2의 예측값은 0.23, 0.77 이다.
    -   해당 알고리즘 기준으로 thread 값 변화에 따라 예측값이 변한다는 의미를 얘기 해주고 싶은건지..?
-   결측값이 있다면 분류 트리는 어떻게 분리 할까?
    -   회귀 트리와 비슷한 방식으로 처리
    -   결측 정보가 없는 샘플만 처리 (8.1 참조)
    -   예측 -&gt; 결측값이 있으면 다른 분기점 사용
-   변수가 연속형이라면?
    -   분할 프로세스는 명확히 보인다.
-   변수가 범주형이라면?
    -   일반적 통계 모형과는 다른 방법. 이항 가변수(1,0)을 만들어 구분하고 모델에 포함시킨다.
    -   새 예측 변수가 하나의 분기점을 갖게 되어 순도를 쉽게 구할 수 있다.
    -   1에 대해 0,1 로 구분하여 하나의 분기점을 만든다는 의미
-   범주형 데이터를 어떻게 처리할지 결정해야 한다.
    -   그룹 범주 방식 - 범주형 변수를 모델에 그냥 넣는 방법. 어떻게 묶을지 설정에서 결정
    -   개별 범주 방식 - 범주형 변수에 이항 가변수를 넣고 그것에 따라 나누는 것.
    -   모델을 돌려보면서 선택해야 한다. 만약 모델 예측력이 높다면 그룹 범주 방식을 사용하는게 좋다.
        -   예전에 범주형 의사결정 나무를 그리면서 개별 범주 방식으로 그렸던 기억이 있다.
-   범주형 변수의 분할 과정
    -   첫번째 분기는 17개 범주를 갖는 계약값 그룹
    -   14.3을 보았을 때, I,J,P,unkonw 그룹 그 이외의 변수로 나누었다.
    -   최적 분할을 결정 전에 범주를 결정하는 것이 합리적이다.
        -   하지만 의사결정 나무는 greedy(탐욕 알고리즘)방식을 사용한다.
    -   greedy(탐욕알고리즘) -&gt; 의사 결정 나무를 분리할 때 눈앞에 있는 범주들의 분리 값을 최우선으로 한다.
        -   마지막에 최선으로 나눠지는 것을 고려하지 않고 나눠보면서 최적의 알고리즘을 찾는다는 의미
        -   그 순간 최선의 답을 찾아나간다고 이해하면 될듯
    -   범주의 비율에 따라 정렬하는 방법
        -   14.2의 상위 그래프는 보조금 지원 성공 확률 의미. 지니계수는 정렬된 범주를 분할 하고 있음.
        -   14.2의 하위 그래프의 지니계수를 살펴보면 3개 범주 추가하는 경우 지니계수가 낮아지는 것을 확인.
        -   I,M 범주 분기에서 지니 계수가 가장 낮게 나올 것으로 파악하고 이 지점에서 분기 되었을 것으로 파악.
        -   I, J, P, Un 으로 구분하고 나머지 그룹으로 분리 하고 실패와 성공으로 분리 하게 된다.

![그림 14.2](https://github.com/topepo/APM_Figures/blob/master/Chapter_14_Classification_Trees_and_Rule-Based_Models/Ch14Fig02.png?raw=true)

![그림 14.3](https://github.com/topepo/APM_Figures/blob/master/Chapter_14_Classification_Trees_and_Rule-Based_Models/Ch14Fig03.png?raw=true)

![그림 14.4](https://github.com/topepo/APM_Figures/blob/master/Chapter_14_Classification_Trees_and_Rule-Based_Models/Ch14Fig04.png?raw=true)

-   14.3과 14.4 그림의 차이
    -   14.3 : 예측 변수를 그룹 범주로 둔 경우. 14.4 CART(classification and regression tree)를 각각 범주형 변수를 사용해 만든 경우
    -   상세 설명
    -   14.3 예측 변수(0,1)이 명확하지 않고 병합되어 있다. 위의 설명 처럼 i,j,p,un 그룹을 실패로 나머지를 성공으로 하여 트리 해석이 어려움.
    -   그러나 예측 변수와 응답 변수 간의 관계를 가지고 파악은 가능
    -   14.4 개별 범주로 나눈 경우(0,1) 최종적으로는 16개 노드를 갖는다. 생각보다 많이 가지치기가 되지는 않았다.16개에 비해..

![그림 14.5](https://github.com/topepo/APM_Figures/blob/master/Chapter_14_Classification_Trees_and_Rule-Based_Models/Ch14Fig05.png?raw=true)

-   14.5 그림상으로 보았을 때 큰 차이는 없어 보인다... 그룹 범주, 개별 범주 다 0.98이다.

-   두 모델 다 몇개 변수는 상위 단계에서 공통되게 사용하는 성향을 보인다.
    -   두 모델 중요한 정보를 비슷하게 정의하나 개별 범주 사용 트리가 해석하기 쉽다.
    -   개별 범주 트리를 사용하면 그룹 범주 사용시에는 어려웠던 예측 변수와 응답변수 간 관계 해석에 용이할듯?

> C4.5 모델

-   정보 이론에 따라 분류 기준을 만듬(1993년 모형)

-   트리를 통해 말단 노드에서 클래스별 확률 분포 등 데이터 정보를 얻는다 가정.

-   정보 통계량
    -   데이터 상에서 해석할 수 있는 비트의 평균 수치
    -   설명 : 확률 분포에 따라 샘플의 불확실성이 변화한다.
-   정보량
    -   c=2 클래스가 있는 경우. 첫번째 클래스 확률은 p
    -   식 -&gt; -\[plog\_2p + (1-p)log\_2(1-p)\]
    -   p=0이 되며 단위는 비트가 된다.
-   그림 14.1을 보았을 때
    -   p=0.53

``` r
p=0.53
-(p*log2(p)+(1-p)*log2((1-p)))
```

    ## [1] 0.9974016

``` r
# 정보량 0.997
p=0.1
-(p*log2(p)+(1-p)*log2((1-p)))
```

    ## [1] 0.4689956

``` r
# 정보량 0.46
```

정보량이 0.45 비트로 작아져 임의로 실제 클래스 추정하기가 쉬워진다.

-   정보량이 분기 분할과 무슨 상관이 있을까?

    -   정보량=-\[(n\_1+/n)`*`log2(n\_1+/n)\] - \[(n\_2+/n)\*log(n\_2+/n)\]
    -   분류 트리에 분기가 생기면 정보 기준이 향상 된다.

-   information gain(정보 획득량)
    -   정보획득량(분기) = 정보량(분기 전) - 정보량(분기 후)
    -   트리는 정보 획득량이 큰 분기를 선호한다.

    -   이항 분기 후, 분기 정보량은 각 분할 지점 정보량 합이다.
    -   정보량 = -\[(n\_11/n\_+1)`*`log2(n\_11/n\_+1)\] - \[(n\_12/n\_+1)\*log2(n\_12/n\_+1)\]

    -   분기 반대 식도 유사 =&gt; 분기 후 전체 정보량은 각 정보량에 대한 가중평균.
    -   정보량(분기 후) = n\_+1/n + n\_+2/n

    -   연속형 변수의 경우 -&gt; 정보 획득량 최대 변수, 단일 분기 찾는 것이 목적.
    -   지니 계수상 최적 분기로 알수 있다

-   문제점
    -   예측 변수상 분기 사용값이 많아질수록 정보 획득량 기준은 값이 많은 쪽으로 치우친다.
    -   회귀 트리 편향성과 유사
-   범주형 변수를 분기 변수로 사용한다면?
    -   여러 분기가 개별값으로 나타나는 다중 분기.
    -   2개의 범주가 하나로 묶이고, 분리 가능 조합에 대한 다중 분기

    -   그리디 알고리즘으로 병합해 최적 변수를 찾는다.

-   결측값 포함 데이터 셋으로 트리를 만들 때 -&gt; C4.5 훈련과정 수정
    -   비 결측값 데이터로 정보 통계량 계산 -&gt; 비결측값 데이터 비율 척도화
    -   예측 변수 정보량 기반으로 조절하여 선택 편향성 처리리
    -   최종 분기를 통해 클래스 분포 결정, 결측값은 클래스 비율에 따라 배치.
    -   ex) 1:2의 비율이면 0.3:0.7로 분리
-   일단은 과적합 트리를 만들고 가지치기
    -   하위트리 제거
    -   하위트리 키워 다른 노드 대체
    -   비용 복잡도에 따라 가지치기 여부 판단
    -   번역이 너무 이상하다...ㅠㅠ
    -   오차 상한선에 따라 트리 제거 여부를 판단한다.
-   추정 오차율 계산
    -   C4.5 에서 0.25 구간 기본 신뢰구간 사용
    -   신뢰요인 증가 -&gt; 트리 커짐 (통계적 기반은 없다...)
    -   트리 가지치기 -&gt; 적합 경로 서치 -&gt; 예측
-   한개 이상의 결측값 가진 샘플
    -   비율 적용
    -   결측값 있는 분기 가능한 모든 경로를 구한다.
    -   예측 클래스는 가장 큰 비율을 차지하는 클래스로 가져온다.
    -   가중값 합치고 총 가중값이 가장 큰 클래스를 예측 클래스로 사용

![그림 14.6](https://github.com/topepo/APM_Figures/blob/master/Chapter_14_Classification_Trees_and_Rule-Based_Models/Ch14Fig06.png?raw=true)

-   그룹, 개별 범주로 그린 그림.
    -   그룹 범주트리가 개별 범주 트리보다 더 많은 노드가 생긴다.
    -   후원재단 코드를 사용한 분기가 생겨서 그렇다.
